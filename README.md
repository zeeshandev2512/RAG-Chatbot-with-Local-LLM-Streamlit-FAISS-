# RAG Chatbot with Local LLM (Streamlit + FAISS)

This project is an AI-powered chatbot that can answer questions based on a custom document using a Retrieval-Augmented Generation (RAG) pipeline. It combines local embeddings, semantic search, and a local LLM via Ollama, all wrapped in a simple Streamlit interface.

---

## ğŸ“„ Project Overview

* **Goal**: Answer user questions based on a provided document (e.g., terms, policies, legal contracts).
* **Tech Stack**: Python, Streamlit, SentenceTransformers, FAISS, Ollama (Mistral), spaCy
* **Key Features**:

  * Document chunking and preprocessing
  * FAISS vector search
  * Prompt formatting for context-aware generation
  * Streamlit interface with real-time response streaming

---

## ğŸ“ Directory Structure

```
â”œâ”€â”€ app.py                     # Streamlit frontend
â”œâ”€â”€ document_chunks.pkl       # Saved text chunks
â”œâ”€â”€ faiss_index.index         # Vector store index
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md
â””â”€â”€ AI Training Document.pdf  # Input document
```

---

## ğŸš€ Demo

### ğŸ”— Watch the Demo Video:

[Demo Video Link Here](https://your-demo-link.com)

### ğŸ”¹ Screenshot or GIF

![Chatbot Demo](https://your-gif-or-demo-link.gif)

---

## ğŸ› ï¸ How It Works

1. **Text Extraction**: PDF is cleaned and split into sentence-based chunks (100â€“300 words).
2. **Embeddings**: Each chunk is embedded using `all-MiniLM-L6-v2`.
3. **Vector Indexing**: FAISS stores all chunk embeddings.
4. **Query Flow**:

   * User enters a question
   * Query is embedded and matched to top-k chunks
   * These are injected into a prompt template
   * Prompt is passed to Ollama (Mistral model) for generation

---

## ğŸ“¥ Installation

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Start the Local LLM

```bash
ollama run mistral
```

### 4. Launch the App

```bash
python -m streamlit run app.py
```

---

## ğŸ’¡ Example Queries

* What are the objectives of the AI training document?
* What models can be used?
* How is the evaluation done?
* What is the meaning of life? *(hallucination example)*

---

## âš™ï¸ Tech Details

* **Embedding Model**: `all-MiniLM-L6-v2`
* **LLM**: Mistral 7B via Ollama
* **Vector DB**: FAISS (L2 distance)
* **Frontend**: Streamlit

---

## âš ï¸ Known Limitations

* No confidence scoring or citation highlighting
* Can hallucinate for off-topic queries
* Performance limited by CPU (slower than GPU LLMs)

---

## ğŸ“Œ To-Do / Future Improvements

* Add source highlighting or reference tags
* Support longer documents with chunk overlap
* Add streaming token-by-token response
* Dockerize the setup for easier deployment

---

## ğŸ™‹ Contact

For questions or feedback, feel free to reach out via [GitHub Issues](https://github.com/yourusername/rag-chatbot/issues).

---

## ğŸ“„ License

This project is open-source and free to use under the MIT License.
